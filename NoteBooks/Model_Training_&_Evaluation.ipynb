{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Dataset & Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qbE-4ZdtY7N",
    "outputId": "150e2d3c-afbb-46e9-a049-df0f1e14daeb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidentGrade</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>EvidenceRole_Others</th>\n",
       "      <th>EvidenceRole_Related</th>\n",
       "      <th>DeviceId_98799</th>\n",
       "      <th>...</th>\n",
       "      <th>CountryCode_242</th>\n",
       "      <th>CountryCode_Others</th>\n",
       "      <th>State_0</th>\n",
       "      <th>State_1</th>\n",
       "      <th>State_1445</th>\n",
       "      <th>State_Others</th>\n",
       "      <th>City_0</th>\n",
       "      <th>City_1</th>\n",
       "      <th>City_10630</th>\n",
       "      <th>City_Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IncidentGrade  Year  Month  Day  Hour  Minute  Second  EvidenceRole_Others  \\\n",
       "0              1  2024      6    4     6       5      15                False   \n",
       "1              2  2024      6   14     3       1      25                 True   \n",
       "2              2  2024      6   13     4      52      55                False   \n",
       "3              0  2024      6   10    16      39      36                False   \n",
       "4              1  2024      6   15     1       8       7                 True   \n",
       "\n",
       "   EvidenceRole_Related  DeviceId_98799  ...  CountryCode_242  \\\n",
       "0                  True            True  ...            False   \n",
       "1                 False            True  ...             True   \n",
       "2                  True            True  ...             True   \n",
       "3                  True            True  ...             True   \n",
       "4                 False            True  ...             True   \n",
       "\n",
       "   CountryCode_Others  State_0  State_1  State_1445  State_Others  City_0  \\\n",
       "0                True    False    False       False          True   False   \n",
       "1               False    False    False        True         False   False   \n",
       "2               False    False    False        True         False   False   \n",
       "3               False    False    False        True         False   False   \n",
       "4               False    False    False        True         False   False   \n",
       "\n",
       "   City_1  City_10630  City_Others  \n",
       "0   False       False         True  \n",
       "1   False        True        False  \n",
       "2   False        True        False  \n",
       "3   False        True        False  \n",
       "4   False        True        False  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../Resources/cleaned_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping unnecessary columns which are not present in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Id_1', 'Id_1751', 'Id_1783', 'Id_Others', 'IncidentId_2', 'IncidentId_7', 'IncidentId_9', 'IncidentId_Others', 'AlertId_0', 'AlertId_2', 'AlertId_3', 'AlertId_Others', 'Url_0', 'Url_1', 'Url_160396', 'Url_Others'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8922805, 89)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 624596\n",
      "Validation set size: 267684\n",
      "\n",
      "Class distribution in training set:\n",
      "IncidentGrade\n",
      "0    0.428991\n",
      "1    0.351920\n",
      "2    0.219089\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation set:\n",
      "IncidentGrade\n",
      "0    0.428991\n",
      "1    0.351922\n",
      "2    0.219087\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Reducing the dataset size\n",
    "sample_fraction = 0.10  # Use only 10% of the dataset\n",
    "df_sampled, _ = train_test_split(\n",
    "    df, \n",
    "    stratify=df['IncidentGrade'], \n",
    "    test_size=1-sample_fraction, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_sampled.drop(columns=['IncidentGrade'])\n",
    "y = df_sampled['IncidentGrade']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Function to apply balancing in chunks\n",
    "def balance_data(X, y, chunk_size):\n",
    "    balanced_chunks = []  # To store balanced chunks\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    smote_enn = SMOTEENN(random_state=42, n_jobs=-1)\n",
    "\n",
    "    for i in range(0, len(X), chunk_size):  # Process data in chunks\n",
    "        X_chunk, y_chunk = X[i:i + chunk_size], y[i:i + chunk_size]\n",
    "\n",
    "        # Step 1: Apply undersampling\n",
    "        X_rus, y_rus = rus.fit_resample(X_chunk, y_chunk)\n",
    "\n",
    "        # Step 2: Apply SMOTEENN to clean noisy samples and further balance\n",
    "        X_res, y_res = smote_enn.fit_resample(X_rus, y_rus)\n",
    "\n",
    "        # Store balanced chunk\n",
    "        balanced_chunks.append((pd.DataFrame(X_res), pd.Series(y_res)))\n",
    "\n",
    "    # Combine all balanced chunks into a single dataset\n",
    "    X_resampled = pd.concat([chunk[0] for chunk in balanced_chunks], ignore_index=True)\n",
    "    y_resampled = pd.concat([chunk[1] for chunk in balanced_chunks], ignore_index=True)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Applying the function to train dataset\n",
    "X_resampled, y_resampled = balance_data(X_train, y_train, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 267946, 1: 219808, 2: 136842})\n",
      "Class distribution after undersampling and SMOTEENN: Counter({2: 29802, 1: 24719, 0: 16925})\n"
     ]
    }
   ],
   "source": [
    "# Print class distributions before and after\n",
    "print(\"Original class distribution:\", Counter(y_train))\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to train dataset\n",
    "X_resampled_test, y_resampled_test = balance_data(X_test, y_test, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 114834, 1: 94204, 2: 58646})\n",
      "Class distribution after undersampling and SMOTEENN: Counter({2: 12723, 1: 10603, 0: 7036})\n"
     ]
    }
   ],
   "source": [
    "# Print class distributions before and after\n",
    "print(\"Original class distribution:\", Counter(y_test))\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_resampled_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Traning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, f1_score,\n",
    "    precision_score, recall_score, confusion_matrix)\n",
    "\n",
    "def evaluate_and_predict(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Train on full training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict on test data\n",
    "    y_val_pred = model.predict(X_test)\n",
    "    \n",
    "    return y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(model_name, y_resampled_test, y_val_pred):\n",
    "    print(f\"{model_name} Performance on Test Set:\")\n",
    "    print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=42, max_iter=2000)\n",
    "y_val_pred = evaluate_and_predict(lr, X_resampled, y_resampled, X_resampled_test, y_resampled_test)\n",
    "\n",
    "# model_performance('Logistic Regression', y_resampled_test, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance on Test Set:\n",
      "Macro-F1 Score: 0.6795\n",
      "Precision: 0.7153\n",
      "Recall: 0.6667\n",
      "Accuracy: 0.6908\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.55      0.60      7036\n",
      "           1       0.86      0.64      0.73     10603\n",
      "           2       0.62      0.81      0.70     12723\n",
      "\n",
      "    accuracy                           0.69     30362\n",
      "   macro avg       0.72      0.67      0.68     30362\n",
      "weighted avg       0.71      0.69      0.69     30362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performance('Logistic Regression', y_resampled_test, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Performance on Test Set:\n",
      "Macro-F1 Score: 0.8741\n",
      "Precision: 0.8741\n",
      "Recall: 0.8741\n",
      "Accuracy: 0.8825\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81      7036\n",
      "           1       0.92      0.91      0.92     10603\n",
      "           2       0.89      0.89      0.89     12723\n",
      "\n",
      "    accuracy                           0.88     30362\n",
      "   macro avg       0.87      0.87      0.87     30362\n",
      "weighted avg       0.88      0.88      0.88     30362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "y_val_pred = evaluate_and_predict(dt, X_resampled, y_resampled, X_resampled_test, y_resampled_test)\n",
    "\n",
    "model_performance('Decision Tree', y_resampled_test, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Performance on Test Set:\n",
      "Macro-F1 Score: 0.8969\n",
      "Precision: 0.8942\n",
      "Recall: 0.9002\n",
      "Accuracy: 0.9036\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84      7036\n",
      "           1       0.94      0.93      0.94     10603\n",
      "           2       0.92      0.90      0.91     12723\n",
      "\n",
      "    accuracy                           0.90     30362\n",
      "   macro avg       0.89      0.90      0.90     30362\n",
      "weighted avg       0.91      0.90      0.90     30362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "y_val_pred = evaluate_and_predict(rf, X_resampled, y_resampled, X_resampled_test, y_resampled_test)\n",
    "\n",
    "model_performance('RandomForestClassifier', y_resampled_test, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "y_val_pred = evaluate_and_predict(xg, X_resampled, y_resampled, X_resampled_test, y_resampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost Performance on Test Set:\n",
      "Macro-F1 Score: 0.8799\n",
      "Precision: 0.8793\n",
      "Recall: 0.8807\n",
      "Accuracy: 0.8879\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      7036\n",
      "           1       0.93      0.92      0.93     10603\n",
      "           2       0.89      0.90      0.90     12723\n",
      "\n",
      "    accuracy                           0.89     30362\n",
      "   macro avg       0.88      0.88      0.88     30362\n",
      "weighted avg       0.89      0.89      0.89     30362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performance('Xgboost', y_resampled_test, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgm\n",
    "\n",
    "lightgbm =  lgm.LGBMClassifier(random_state=42)\n",
    "y_val_pred = evaluate_and_predict(xg, X_resampled, y_resampled, X_resampled_test, y_resampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm Performance on Test Set:\n",
      "Macro-F1 Score: 0.8799\n",
      "Precision: 0.8793\n",
      "Recall: 0.8807\n",
      "Accuracy: 0.8879\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      7036\n",
      "           1       0.93      0.92      0.93     10603\n",
      "           2       0.89      0.90      0.90     12723\n",
      "\n",
      "    accuracy                           0.89     30362\n",
      "   macro avg       0.88      0.88      0.88     30362\n",
      "weighted avg       0.89      0.89      0.89     30362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performance('lightgbm', y_resampled_test, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperparameter Tuning with RandomizedSearchCV (for Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],         # Number of trees\n",
    "    'max_features': ['sqrt', 'log2'],        # Features considered for splitting\n",
    "    'max_depth': [10, 20, None],             # Depth of trees\n",
    "    'min_samples_split': [2, 5],             # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2],              # Minimum samples at a leaf node\n",
    "    'bootstrap': [True]                      # Use of bootstrap samples\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,           # Number of iterations to search\n",
    "    scoring='accuracy',  # Evaluation metric\n",
    "    cv=3,                # Cross-validation folds\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1            # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Hypertuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the model and parameters for tuning\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [200],\n",
    "    'max_features': ['log2'],\n",
    "    \"max_depth\": [None],\n",
    "    \"min_samples_split\": [5],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"bootstrap\": [True]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "rf_best_model = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,           # Number of iterations to search\n",
    "    scoring='accuracy',  # Evaluation metric\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1            # Use all available cores\n",
    ")\n",
    "\n",
    "rf_best_model.fit(X_resampled, y_resampled)\n",
    "y_val_pred = rf_best_model.predict(X_resampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance after hypertuning on Test Set:\n",
      "Macro-F1 Score: 0.8979\n",
      "Precision: 0.8950\n",
      "Recall: 0.9017\n",
      "Accuracy: 0.9043\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      7036\n",
      "           1       0.94      0.93      0.94     10603\n",
      "           2       0.92      0.90      0.91     12723\n",
      "\n",
      "    accuracy                           0.90     30362\n",
      "   macro avg       0.89      0.90      0.90     30362\n",
      "weighted avg       0.91      0.90      0.90     30362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Performance after hypertuning on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in pickle file\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('../Resources/best_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(rf_best_model, model_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbqYDMC1kOYxiUNm4o/pqD",
   "mount_file_id": "112yOxEmQ4qFzNIFMqrmQpOIuO5pdNxzf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
